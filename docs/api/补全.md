# 补全

给定一个提示，该模型将返回一个或多个预测的完成，并且还可以返回每个位置的替代标记的概率。

## 创建补全

`POST` `https://api.openai.com/v1/completions`

为提供的提示和参数创建补全

### 请求正文

**model** `String` `必填` 

要使用的模型的 ID。您可以使用 [List models](/api/API.html#模型列表) API 来查看所有可用模型，或查看我们的[模型](/start/模型)概述以了解它们的描述。

------

**prompt** `string or array` `可选 默认 <|endoftext|>`

生成补全的提示，编码为字符串、字符串数组、标记数组或标记数组数组。

请注意，<\|endoftext\|> 是模型在训练期间看到的文档分隔符，因此如果未指定提示，模型将生成新文档的开头。

------

**suffix** `string` `可选 默认 null`

插入文本完成后出现的后缀。

------

**max_tokens** `integer` `可选 默认 16`

补全时生成的最大[标记](https://platform.openai.com/tokenizer)数。

您提示的标记计数加上 `max_tokens` 不能超过模型的上下文长度。大多数模型的上下文长度为 2048 个标记（最新模型除外，它支持 4096）。

------

**temperature** `number` `可选 默认 1`

使用什么采样`temperature`，介于 0 和 2 之间。较高的值（如 0.8）将使输出更加随机，而较低的值（如 0.2）将使输出更加集中和确定。

我们通常建议改变这个或 `top_p` 但不是两者都改变。

------

**top_p** `number`  `可选 默认 1`

一种替代 `temperature` 采样的方法，称为核采样，其中模型考虑具有 `top_p` 概率质量的标记的结果。所以 0.1 意味着只考虑构成前 10% 概率质量的标记。

我们通常建议更改此值或 `temperature`，但不要同时更改两者。

------

**n** `integer` `可选 默认 1`

为每个提示生成多少补全。

**注意**：因为这个参数会产生很多补全，它会很快消耗你的标记配额。请谨慎使用并确保您对 `max_tokens` 和`stop`进行了合理的设置。

------

**stream** `boolean` `可选 默认 false`

是否回流部分进度。如果设置，标记将在可用时作为纯数据服务器[发送事件](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)发送，流数据全部返回后，数据返回:`[NODE]` 结束。

说白了就已流的形式返回结果，不再是`JSON`格式了

------

**logprobs** `integer` `可选 默认 null`

在 `logprobs` 上包括对数概率最有可能的标记，以及所选标记。例如，如果 `logprobs` 为 5，API 将返回 5 个最有可能的标记的列表。 API 将始终返回采样标记的 `logprob`，因此响应中最多可能有`logprobs+1` 个元素。

`logprobs` 的最大值为 5。如果您需要更多，请通过我们的[帮助中心](https://help.openai.com/)联系我们并描述您的用例。

------

**echo** `boolean` `可选 默认 false`

除了补全之外回显提示

------

**stop** `string or array`  `可选 默认 null`

API 将停止生成更多标记的最多 4 个序列。返回的文本将不包含停止序列。

------

**presence_penalty** `number` `可选 默认 0`

-2.0 和 2.0 之间的数字。正值会根据到目前为止是否出现在文本中来处罚新标记，从而增加模型谈论新题目的可能性。

[查看有关频率和存在惩罚的更多信息](#参数详情)。

-----

**frequency_penalty** `number` `可选 默认 0`

-2.0 和 2.0 之间的数字。正值会根据新标记在文本中的现有频率对其进行处罚，从而降低模型逐字重复同一行的可能性。

[查看有关频率和存在惩罚的更多信息](#参数详情)。

------

**best_of** `integer` `可选 默认 1`

在服务器端生成 `best_of` 补全并返回“最佳”（每个标记具有最高对数概率的那个）。无法流式传输结果。当与 `n` 一起使用时，`best_of` 控制候选补全的数量，n 指定返回多少 - `best_of` 必须大于 `n`。

**注意**：因为这个参数会产生很多补全，它会很快消耗你的标记配额。请谨慎使用并确保您对 `max_tokens` 和 `stop`进行了合理的设置。

------

**logit_bias** `map` 可选 默认 null

修改指定标记出现在补全中的可能性。

接受一个 json 对象，该对象将令牌（由 GPT 分词器中的令牌 ID 指定）映射到从 -100 到 100 的相关偏差值。您可以使用此[标记生成器工具](https://platform.openai.com/tokenizer?view=bpe)（适用于 GPT-2 和 GPT-3）将文本转换为标记 ID。从数学上讲，偏差会在采样之前添加到模型生成的对数中。确切的效果因模型而异，但 -1 和 1 之间的值应该会减少或增加选择的可能性；像 -100 或 100 这样的值应该导致相关标记的禁止或独占选择。

例如，您可以传递 `{"50256": -100}` 以防止生成 <|endoftext|> 标记。

------

**user** `string` `可选`

代表您的最终用户的唯一标识符，可以帮助 OpenAI 监控和检测滥用行为。[了解更多](/guides/安全最佳实践.html#最终用户-id)。

-------

### 请求示例

**curl**

```shell
curl https://api.openai.com/v1/completions \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_API_KEY' \
  -d '{
  "model": "text-davinci-003",
  "prompt": "Say this is a test",
  "max_tokens": 7,
  "temperature": 0
}'
```

**参数**

```json
{
  "model": "text-davinci-003",
  "prompt": "Say this is a test",
  "max_tokens": 7,
  "temperature": 0,
  "top_p": 1,
  "n": 1,
  "stream": false,
  "logprobs": null,
  "stop": "\n"
}
```

**响应结果**

```json
{
  "id": "cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7",
  "object": "text_completion",
  "created": 1589478378,
  "model": "text-davinci-003",
  "choices": [
    {
      "text": "\n\nThis is indeed a test",
      "index": 0,
      "logprobs": null,
      "finish_reason": "length"
    }
  ],
  "usage": {
    "prompt_tokens": 5,
    "completion_tokens": 7,
    "total_tokens": 12
  }
}
```

